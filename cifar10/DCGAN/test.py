from __future__ import print_function

import argparse
import os

import matplotlib.pyplot as plt
import model as model_
import numpy as np
import torch
import torch.utils.data
import torchvision.transforms as transforms
from numpy.lib.stride_tricks import as_strided
from scipy.stats import entropy
from torch import nn
from torch.autograd import Variable
from torch.nn import functional as F
from torchvision.models.inception import inception_v3


def strided_app(a, L, S):
	nrows = ((len(a) - L) // S) + 1
	n = a.strides[0]
	return as_strided(a, shape=(nrows, L), strides=(S * n, n))


def inception_score(model, N=1000, cuda=True, batch_size=32, resize=False, splits=1):
	"""
	adapted from: https://github.com/sbarratt/inception-score-pytorch/blob/master/inception_score.py
	Computes the inception score of images generated by model
	model -- Pretrained Generator
	N -- Number of samples to test
	cuda -- whether or not to run on GPU
	batch_size -- batch size for feeding into Inception v3
	splits -- number of splits
	"""

	assert batch_size > 0
	assert N > batch_size

	# Set up dtype
	if cuda:
		dtype = torch.cuda.FloatTensor
	else:
		if torch.cuda.is_available():
			print("WARNING: You have a CUDA device, so you should probably set cuda=True")
		dtype = torch.FloatTensor

	# Load inception model
	inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)
	inception_model.eval()
	up = nn.Upsample(size=(299, 299), mode='bilinear').type(dtype)

	def get_pred(N_s):

		z_ = torch.randn(N_s, 100).view(-1, 100, 1, 1)

		if cuda:
			z_ = z_.cuda()

		z_ = Variable(z_)

		x = model.forward(z_)

		if resize:
			x = up(x)
		x = inception_model(x)
		return F.softmax(x, dim=1).data.cpu().numpy()

	indexes = strided_app(np.arange(N), batch_size, batch_size)

	N = indexes[-1][-1] + 1

	# Get predictions
	preds = np.zeros((N, 1000))

	for i, idx in enumerate(indexes, 0):
		batch_size_i = idx.shape[0]

		preds[i * batch_size:i * batch_size + batch_size_i] = get_pred(batch_size_i)

	# Now compute the mean kl-div
	split_scores = []

	for k in range(splits):
		part = preds[k * (N // splits): (k + 1) * (N // splits), :]
		py = np.mean(part, axis=0)
		scores = []
		for i in range(part.shape[0]):
			pyx = part[i, :]
			scores.append(entropy(pyx, py))
		split_scores.append(np.exp(np.mean(scores)))

	return np.mean(split_scores), np.std(split_scores)


def denorm(unorm):
	norm = (unorm + 1) / 2

	return norm.clamp(0, 1)


def test_model(model, n_tests, cuda_mode):
	model.eval()

	to_pil = transforms.ToPILImage()
	to_tensor = transforms.ToTensor()

	z_ = torch.randn(n_tests, 100).view(-1, 100, 1, 1)

	if cuda_mode:
		z_ = z_.cuda()

	z_ = Variable(z_)
	out = model.forward(z_)

	for i in range(out.size(0)):
		sample = denorm(out[i].data)
		sample = to_pil(sample.cpu())
		sample.save('sample_{}.png'.format(i + 1))


def save_samples(generator, cp_name, cuda_mode, save_dir='./', fig_size=(5, 5)):
	generator.eval()

	n_tests = fig_size[0] * fig_size[1]

	noise = torch.randn(n_tests, 100).view(-1, 100, 1, 1)

	if cuda_mode:
		noise = noise.cuda()

	noise = Variable(noise, volatile=True)
	gen_image = generator(noise)
	gen_image = denorm(gen_image)

	generator.train()

	n_rows = np.sqrt(noise.size()[0]).astype(np.int32)
	n_cols = np.sqrt(noise.size()[0]).astype(np.int32)
	fig, axes = plt.subplots(n_rows, n_cols, figsize=fig_size)
	for ax, img in zip(axes.flatten(), gen_image):
		ax.axis('off')
		ax.set_adjustable('box-forced')
		# Scale to 0-255
		img = (((img - img.min()) * 255) / (img.max() - img.min())).cpu().data.numpy().transpose(1, 2, 0).astype(np.uint8)
		# ax.imshow(img.cpu().data.view(image_size, image_size, 3).numpy(), cmap=None, aspect='equal')
		ax.imshow(img, cmap=None, aspect='equal')
	plt.subplots_adjust(wspace=0, hspace=0)
	title = 'Samples'
	fig.text(0.5, 0.04, title, ha='center')

	# save figure

	if not os.path.exists(save_dir):
		os.mkdir(save_dir)
	save_fn = save_dir + 'Cifar10_DCGAN_' + cp_name + '.png'
	plt.savefig(save_fn)

	plt.close()


def plot_learningcurves(history, *keys):
	for key in keys:
		plt.plot(history[key])

	plt.show()


if __name__ == '__main__':

	# Testing settings
	parser = argparse.ArgumentParser(description='Testing GANs under max hyper volume training')
	parser.add_argument('--cp-path', type=str, default=None, metavar='Path', help='Checkpoint/model path')
	parser.add_argument('--n-tests', type=int, default=4, metavar='N', help='number of samples to generate (default: 4)')
	parser.add_argument('--no-plots', action='store_true', default=False, help='Disables plot of train/test losses')
	parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables GPU use')
	parser.add_argument('--inception', action='store_true', default=False, help='Enables computation of the inception score over the test set of cifar10')
	parser.add_argument('--n-inception', type=int, default=1024, metavar='N', help='number of samples to calculate inception score (default: 1024)')
	args = parser.parse_args()
	args.cuda = True if not args.no_cuda and torch.cuda.is_available() else False

	if args.cp_path is None:
		raise ValueError('There is no checkpoint/model path. Use arg --cp-path to indicate the path!')

	model = model_.Generator(100, [1024, 512, 256, 128], 3)

	ckpt = torch.load(args.cp_path, map_location=lambda storage, loc: storage)
	model.load_state_dict(ckpt['model_state'])

	if args.cuda:
		model = model.cuda()

	print('Cuda Mode is: {}'.format(args.cuda))

	history = ckpt['history']

	if not args.no_plots:
		plot_learningcurves(history, 'gen_loss')
		plot_learningcurves(history, 'disc_loss')
		plot_learningcurves(history, 'gen_loss_minibatch')
		plot_learningcurves(history, 'disc_loss_minibatch')
		plot_learningcurves(history, 'FID-c')

	test_model(model=model, n_tests=args.n_tests, cuda_mode=args.cuda)
	save_samples(generator=model, cp_name=args.cp_path.split('/')[-1].split('.')[0], cuda_mode=args.cuda)

	if args.inception:
		print(inception_score(model, N=args.n_inception, cuda=args.cuda, resize=True, splits=10))
